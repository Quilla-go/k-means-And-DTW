## k-means-And-DTW原理及算法介绍

Clustering and fitting of time series based on DTW and k-means

[TOC]

### 一、DTW原理介绍

#### 1.1 传统k-mean算法及存在的问题

简单写一个测试数据（代码里的`get_draw()`函数）：

```python
# 展示一下如何使用plot绘图。
def get_draw():
    x = np.linspace(0, 50, 100)
    ts1 = pd.Series(3.1 * np.sin(x / 1.5) + 3.5)
    ts2 = pd.Series(2.2 * np.sin(x / 3.5 + 2.4) + 3.2)
    ts3 = pd.Series(0.04 * x + 3.0)
    ts1.plot()
    ts2.plot()
    ts3.plot()
    plt.ylim(-2, 10)
    plt.legend(['ts1', 'ts2', 'ts3'])
    plt.show()
```

`ts1`是窄的正弦波，`ts2`是宽的正弦波，`ts3`是直线，可以从图看出：



![img](chapter01.assets/1704791-20200315235704173-1679457526.png)



如果采用欧拉距离进行聚类计算，则会得到`ts3`比`ts2`更接近`ts1`的结论。但是肉眼看并非如此。

因此学者提出``DTW``距离。

#### 1.2 动态时间规整（Dynamic Time Warping, `DTW`）

动态时间规整算法，故名思议，就是把两个代表同一个类型的事物的不同长度序列进行时间上的“对齐”。

![](chapter01.assets/20150515183704204.jpeg)

比如``DTW``最常用的地方，是在语音识别中，同一个字母，由不同人发音，长短肯定不一样，把声音记录下来以后，它的信号肯定是很相似的，只是在时间上不太对整齐而已。所以我们需要用一个函数拉长或者缩短其中一个信号，使得它们之间的误差达到最小。

这篇博文给了比较好的解释[^1]：

>   ##### 1、动态时间规整（`DTW`）基本思想
>
>   动态时间规整：Dynamic Time Warping（`DTW`），是一种衡量两个离散时间序列相似度的方法，主要特点是在序列长度不一或x轴无法完全对齐的情况下，用满足一定条件的的时间规整函数描述两者之间的时间对应关系。`DTW`算法在各种模式匹配任务中被广泛使用，如语音识别、动态手势识别和信息检索等中。
>
>   假设下图两个时间序列对应的是同一个单词的发音（实则只是示意图，只是为了便于理解）。
>
>   ![图1 时间序列数据规整前](chapter01.assets/70.png)
>
>   图1↑
>
>   
>
>   `图1`里黑色的线表示两个时间序列的相似的点（用幅度差异刻画时间序列点的相似性，幅度差异越小，相似性越高）。
>
>   由于不同人语速上存在差异，某些人可能会把字母`A`发得很长（延长发音），某些人却发得较短（短促发音），这样同一个字母展现出来的时间序列上就存在着很大的差异，如图中绿色圈出的时间波形所示。
>
>   因此就需要对原始的两个时间序列进行规整，即对时间序列进行延伸和缩短，提取两个时间序列的相似性，从而对语音进行识别。
>
>   ![图2 时间序列数据规整后](chapter01.assets/70-16377268295841.png)
>
>   图2↑
>
>   
>
>   `图2`对应的就是原始时间序列的规整后的结果。
>
>   所以动态时间规整的思想就是：通过对两个时间序列点之间的相似性进行比较（图1黑线），对原始时间序列进行拉伸到相同时间长度（原始时间序列的长度很可能不一致），进而比较两个时间序列的相似性。
>
>   动态时间规整要解决的问题就是，找到一条最优的规整路径：
>
>   $W = {\varpi_1},{\varpi_2}...{\varpi_k}，其中{\varpi_k} = (i,j)$，
>
>   即认为`时间序列1`的第`i`个点和`时间序列2`的第`j`个点是相似的。
>
>   所有相似点的距离之和作为规整路径距离，用规整路径距离来衡量两个时间序列的相似性。规整路径距离越小，相似度越高。
>
>   主要步骤如下：
>
>
>   1. 假定两个待匹配的离散数据分别为 $A = {A(1), A(2), …, A(m)}$和 $B = {B(1), B(2), …, B(m)}$，其中下标为1的元素为序列的起点，下标为 $m$ 或者 $n$ 的元素为序列终点。
>   2. 采用了“动态规划”的方法对齐`A,B`两个序列.首先构造一个$m × n$的矩阵，用于存放两序列点对点之间的距离（一般可使用欧氏距离），距离越小表明两点之间的相似度越高。
>   3. 该部分是DTW算法的核心。把$m × n$的矩阵看成一个网格，算法的目的可总结为寻找一条通过此矩阵网格的最优路径，该路径通过的网格点即为两个离散序列经过对齐后的点对。
>   4. 找到最优路径后，DTW算法定义了一个归整路径距离(Warp Path Distance)，通过使用所有相似点之间距离的和，来衡量两个时间序列之间的相似性。
>
>   
>
>
>   ##### 2、动态时间规整（`DTW`）解释
>
>   假设原始时间序列为$X,Y$，它们的时间长度分别为$∣ X ∣和∣ Y ∣$。对于规整路径$W = {\varpi _1},{\varpi _2}...{\varpi _k}$，有：
>   $$
>   max(∣X∣,∣Y∣)≤k≤∣X∣+∣Y∣
>   $$
>
>   $k$表示两个序列最终被拉伸的长度。
>
>   规整路径必须从$ {\varpi _1} = (1,1)$开始，到$ {\varpi _k} = (\left| X \right|,\left| Y \right|)$结束，以保证$X$和$Y$序列的每个坐标点都出现一次。另外，规整路径${w_k} = (i,j)$中的$i$和$j$必须是单调递增的。
>
>   也就是说，路径要满足：
>
>   1、边界条件：$w_1=(1, 1)和W_k=(m, n)$。
>   2、连续性：如果$W_{k-1} = (i, j)$，那么对于路径的下一个点$w_k=(i^′,j^′)$需要满足：$(i^′ - i) \leq 1 和 (j^′-j) \leq 1$。因此只能和自己相邻的点对齐。这样可以保证序列A和B中的每个元素都在规整路径W中出现。
>   3、单调性：如果$W_{k-1} = (i, j)$，那么对于路径的下一个点$w_k=(i^′,j^′)$需要满足：$(i^′- i) \geq 0 和 (j^′ - j) \geq 0$。在这里需要假设A和B的顺序均是不可改变的。因此路径W在矩阵网格中的走势必须是随着时间单调递增的，如下图所示，整个过程是从矩阵网格的左下角出发，在右上角结束。
>
>   ![](chapter01.assets/20150515185351326.jpeg)
>
>   单调递增也可以写为：
>   $$
>   w_k = (i, j), w_{k+1} = (i^′, j^′)　　　　i≤i^′≤i+1, j≤j^′≤j+1
>   $$
>
>   所以如果路径已经通过了格点$(i, j)$，那么路径的下一个格点只能是$(i+1,j)，(i, j+1)，(i+1, j+1)$中的一种，如`图3`中绿色剪头所示。
>
>   ![图3](chapter01.assets/70-16377268295842.png)
>
>   图3↑
>
>   
>
>   基于以上3个约束，规整路径的计算被简化为三种情况，或者说三个方向：
>
>   ![这里写图片描述](chapter01.assets/20150515183127473.jpeg)
>
>   最后，需要定义一个累加距离`dist`，即从`(0,0)`点开始匹配两个序列A和B，每到一个点，之前所有的点计算的距离**都会累加**。到达终点`(n,m)`后，这个累积距离就描述了序列A和B的总体相似程度。
>
>   累积距离$dist(i,j)$可以表示成以下公式：
>
>   ![这里写图片描述](chapter01.assets/20150515184715237.jpeg)
>
>   
>
>   所以对于路径规整距离矩阵$D ( i , j )$，有：
>   $$
>   D(i,j) = Dist(i,j) + \min \{ D(i - 1,j),D(i,j - 1),D(i - 1,j - 1)\} \tag{3}
>   $$
>
>   其中，
>
>   $Dist(i, j)$表示$X$序列第$i$个点与$Y$序列第$j$个点之间的距离（两个点的相似性）。 
>
>   $D(i, j)$衡量的是$X$序列前$i$个点与$Y$序列前$j$个点的相似性。 
>
>   最终的规整路径距离为$D(\left| X \right|,\left| Y \right|)$。
>
>   $D(\left| X \right|,\left| Y \right|)$的值越小，两个原始时间序列的相似性越大。
>
>   有了式子(3)，我们就可以用动态规划来对`DTW`问题进行求解。
>
>   设定不一样的采样频率对正弦函数进行采样，得到两个原始时间序列如`图4`所示。
>
>   ![图4](chapter01.assets/70-16377268295843.png)
>
>   图4↑
>
>   其中`时间序列1`的前半段采样频率低于`时间序列2`前半段的采样频率，后半段高于`时间序列2`的采样频率。利用动态规划的递推公式(3)可以求得路径规整矩阵$D(i, j)$如`图5`所示。
>
>   ![图5](chapter01.assets/70-16377268295844.png)
>
>   图5↑
>
>   由$D(i, j)$可得规整路径如`图6`所示。
>
>   ![图6](chapter01.assets/70-16377268295845.png)
>
>   图6↑
>
>   规整路径规定了时间序列$X$与时间序列$Y$的时间对齐方式。由此可以得到如`图7`所示的规整时间序列。可见，在`DTW`的作用下，完成了两个时间序列的规整目标。
>
>   ![图7](chapter01.assets/70-16377268295856.png)
>
>   图7↑
>
>   `图7`所示的规整序列基本达到了时间序列的对齐目的。可是，如果有两个时间序列的幅值不同，将会产生什么样的规整结果？
>
>   
>
>   `图9`是对`图8`进行规整得到的规整时间序列。由此可知，最终的效果并不理想。
>
>   ![图8 具有不同幅值的两个原始时间序列](chapter01.assets/70-16377268295857.png)
>
>   ![图9 不同幅值时间序列的规整结果](chapter01.assets/70-16377268295858.png)
>
>   为了使得`DTW`能够提取原始序列的时间特征而忽略幅值对序列规整的影响，加入了zscore对原始数据进行标准化。最终的结果如`图10`所示。
>
>   ![图10 加入zscore标准化处理后的规整结果](chapter01.assets/70-16377268295859.png)
>
>   ##### 3、一个简单的单词语音音频识别实例
>
>   ###### 3.1、语音时间序列规整
>
>   自己录制的几段单词mp3文件来探讨动态时间规整算法对语音时间序列的对齐。
>
>   ![图11 两段’water’音频的原始时间序列](chapter01.assets/70-163772682958510.png)
>
>   `图11`所示为两段‘water’这个单词的语音时间序列。
>
>   ![图12 语音时间序列规整](chapter01.assets/70-163772682958511.png)
>
>   由于语速和发音时刻的差异，两个原始序列之间存在差异，但明显可以看出两者之间有很大的相似性。
>
>   利用第2小节阐述的`DTW`算法，最终的语音规整结果如`图12`所示，基本达到了目的。当然这边做的非常粗糙，比如语音里面的很多毛刺，低频信号完全可以用滤波的方式先消掉，然后再交由`DTW`进行处理。
>
>   ###### 3.2、单词语音音频识别
>
>   如何利用`DTW`算法来做简单的语音音频识别？
>
>   第2小节已经提及，`DTW`做序列规整时利用规整路径距离$D(\left| X \right|,\left| Y \right|)$来衡量时间序列$X$和时间序列$Y$的相似性。所以，假设我们现有`water`，`teacher`，`apple`的几段音频序列，要识别某一个音频的发音到底是这三个单词中的哪一个，就只需要将这个待识别音频序列分别与三个单词的音频序列做规整，得到各自的规整路径距离$D(\left| X \right|,\left| Y \right|)$（即附录代码中的变量Dist）。$D(\left| X \right|,\left| Y \right|)$越小，说明两者的相似度越高。
>
>   这样可以初步完成单词语音音频识别的任务。
>
>   当然，实际上进行这样操作的复杂度是很高的，实用性很差。
>
>   ##### 4、总结
>
>   　　① 动态时间规整算法（`DTW`）是一种时间序列对齐方法。它通过寻找一条规整路径来使得规整距离最小。规整路径距离$D(\left| X \right|,\left| Y \right|)$表征了两个时间序列的相似性：$D(\left| X \right|,\left| Y \right|)$越小，相似度越高。
>
>   　　② 可以利用`DTW`算法来做单词音频的识别。



简而言之，就是允许错开求差值，并且取最小的那个作为距离。

`DTW`的问题：

1、运算量大；

2、识别性能过分依赖于端点检测；

3、太依赖于说话人的原来发音；

4、不能对样本作动态训练；

5、没有充分利用语音信号的时序动态特性；

6、`DTW`适合于特定人基元较小的场合，多用于孤立词识别。





#### 1.4  使用k-means算法实现聚类





#### 1.5 根据聚类打印出具体分类情况



### 四、结果

定义了分成两类的情形，可以根据num_clust 的值进行灵活的调整，等于2是的分类和图示情况如下：

WBC01：[6774, 7193, 8070, 8108, 8195, 2020006799, 2020007003, 2020007251, 2020007420, 2020007636, 2020007718, 2020007928, 2020007934, 2020008022, 2020008196, 2020008239, 2020008302, 2020008354, 2020008418, 2020008513, 2020008535, 2020008737, 2020008890, 2020008909, 2020009042, 2020009043, 2020009050, 2020009201, 2020009213, 2020009289, 2020009420, 2020009557]

WBC02：[2020007250, 2020007388, 2020007389, 2020007422, 2020007625, 2020007703, 2020007927, 2020009049, 2020009158, 2020009284, 2020009580]

说明：
代码训练过程中，一定要注意数据类型，比如matrix和ndarray,虽然打印的时候都是（45，30），但是再训练的时候，稍加不注意，就会导致乱七八糟的问题，需要打印排查好久。
本文的数据和代码，请登录：my github，进行下载。若是对您有用，请不吝给颗星。
具体请看博文：https://www.cnblogs.com/yifanrensheng/p/12501238.html

[^1]: https://blog.csdn.net/lin_limin/article/details/81241058
[^2]: https://zhuanlan.zhihu.com/p/86924746

